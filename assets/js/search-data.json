{
  
    
        "post0": {
            "title": "Dive Into Deep Learning",
            "content": "In this module, we are going to develop a neural network to perform image classification for three categories of insects which they are bettles, cockroach, and dragonflies. . This blog post serves to be a presentation of explaining how the codes work to classify the images. The entire set of codes can be retrieved a Jupyter notebook from Pu&#39;s BIOS823 GitHub Page where it&#39;s titiled as DP_Image_Prediction. . We will be using the following librabries to perform this task. . from collections import Counter import shap import numpy as np import pandas as pd import tensorflow as tf import matplotlib.pyplot as plt from PIL import Image import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout from tensorflow.keras.layers.experimental.preprocessing import Rescaling from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.losses import sparse_categorical_crossentropy from tensorflow.keras.optimizers import Adam . Load the Datasets . Images that are used to train and test were obtained from this website, https://www.insectimages.org/index.cfm and were packed into a zipped folder in dropbox. . To get a copy in your container or local machine, you can perform the following commands in terminal or Unix shell: . wget https://people.duke.edu/~ccc14/insects.zip unzip insects.zip ls -R insects . We are going to split the training data into a set of data for training and the other set of data for validation by the ration of 0.2 (i.e 80% of training data and 20% of validation data). Once the model is trained, we will be using the model to assess its performance on the test set. . Thus, we will have three sets of data which they are training set, validation set, and testing set. In addition, each sets of data will have a batch size of 32. In order to speed up the training process, we downsize the images to size of 192 x 192 for the reason that majority of the images have size of 384 x 384 and half of 384 is 192. . The following chunk shows how the training, validation, and testing images are loaded and parsed. . train_set_path = &#39;insects/train&#39; test_set_path = &#39;insects/test&#39; img_size = (384//2, 384//2) batch_size = 32 train_imgs = tf.keras.utils.image_dataset_from_directory(train_set_path, validation_split = 0.2, subset = &#39;training&#39;, batch_size = batch_size, seed = 123, image_size = img_size) val_imgs = tf.keras.utils.image_dataset_from_directory(train_set_path, validation_split = 0.2, subset = &#39;validation&#39;, batch_size = batch_size, seed = 123, image_size = img_size) test_imgs = tf.keras.utils.image_dataset_from_directory(test_set_path, batch_size = batch_size, seed = 123, image_size = img_size) . Found 1019 files belonging to 3 classes. Using 816 files for training. Found 1019 files belonging to 3 classes. Using 203 files for validation. Found 180 files belonging to 3 classes. . Model Architecture . We will be using a CNN network as our model&#39;s architecture. . input_shape = (384//2, 384//2, 3) model = Sequential([ Rescaling(1./255, input_shape = input_shape), Conv2D(32, 3, padding =&#39;same&#39;, activation = &#39;relu&#39;), MaxPooling2D(), Conv2D(32, 3, padding = &#39;same&#39;, activation = &#39;relu&#39;), MaxPooling2D(), Conv2D(32, 3, padding = &#39;same&#39;, activation = &#39;relu&#39;), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(128, activation=&#39;relu&#39;), Dropout(0.2), Dense(3, activation = &#39;softmax&#39;) ]) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= rescaling (Rescaling) (None, 192, 192, 3) 0 conv2d (Conv2D) (None, 192, 192, 32) 896 max_pooling2d (MaxPooling2D (None, 96, 96, 32) 0 ) conv2d_1 (Conv2D) (None, 96, 96, 32) 9248 max_pooling2d_1 (MaxPooling (None, 48, 48, 32) 0 2D) conv2d_2 (Conv2D) (None, 48, 48, 32) 9248 max_pooling2d_2 (MaxPooling (None, 24, 24, 32) 0 2D) dropout (Dropout) (None, 24, 24, 32) 0 flatten (Flatten) (None, 18432) 0 dense (Dense) (None, 128) 2359424 dropout_1 (Dropout) (None, 128) 0 dense_1 (Dense) (None, 3) 387 ================================================================= Total params: 2,379,203 Trainable params: 2,379,203 Non-trainable params: 0 _________________________________________________________________ . In order reduce the access speed, we will use buffered prefetching since the images will be stored into the cache before processing those images. . AUTOTUNE = tf.data.experimental.AUTOTUNE train_imgs = train_imgs.cache().shuffle(500).prefetch(buffer_size = AUTOTUNE) val_imgs = val_imgs.cache().prefetch(buffer_size = AUTOTUNE) test_imgs = test_imgs.cache().prefetch(buffer_size = AUTOTUNE) . In terms of the loss function and optimer in our mode, we will be using sparse cross entropy to evaluate the loss function. And the optimizer, Adam which is a stochastic gradient descent method will be used in compilation. Since this is a prediction task, we will be evaluating the accuracy of the model. . loss_function = sparse_categorical_crossentropy optimizer = Adam() model.compile(loss = loss_function, optimizer = optimizer, metrics = [&#39;accuracy&#39;]) . We will train our model iteratively 25 times. . Train Model . np.random.seed(0) tf.random.set_seed(0) num_epochs = 25 history = model.fit(train_imgs, validation_data = val_imgs, epochs = num_epochs, verbose = 1 ) . Epoch 1/25 26/26 [==============================] - 19s 591ms/step - loss: 1.0564 - accuracy: 0.5245 - val_loss: 0.6479 - val_accuracy: 0.7291 Epoch 2/25 26/26 [==============================] - 13s 488ms/step - loss: 0.6538 - accuracy: 0.7353 - val_loss: 0.4762 - val_accuracy: 0.8030 Epoch 3/25 26/26 [==============================] - 13s 484ms/step - loss: 0.4723 - accuracy: 0.8150 - val_loss: 0.4591 - val_accuracy: 0.8079 Epoch 4/25 26/26 [==============================] - 13s 485ms/step - loss: 0.3995 - accuracy: 0.8640 - val_loss: 0.3151 - val_accuracy: 0.8621 Epoch 5/25 26/26 [==============================] - 14s 539ms/step - loss: 0.3352 - accuracy: 0.8640 - val_loss: 0.2712 - val_accuracy: 0.9015 Epoch 6/25 26/26 [==============================] - 13s 490ms/step - loss: 0.2589 - accuracy: 0.9007 - val_loss: 0.2882 - val_accuracy: 0.8966 Epoch 7/25 26/26 [==============================] - 13s 504ms/step - loss: 0.1928 - accuracy: 0.9301 - val_loss: 0.2374 - val_accuracy: 0.9261 Epoch 8/25 26/26 [==============================] - 13s 487ms/step - loss: 0.1617 - accuracy: 0.9350 - val_loss: 0.2001 - val_accuracy: 0.9113 Epoch 9/25 26/26 [==============================] - 13s 487ms/step - loss: 0.1166 - accuracy: 0.9485 - val_loss: 0.2545 - val_accuracy: 0.9015 Epoch 10/25 26/26 [==============================] - 13s 494ms/step - loss: 0.0926 - accuracy: 0.9669 - val_loss: 0.2383 - val_accuracy: 0.9015 Epoch 11/25 26/26 [==============================] - 13s 484ms/step - loss: 0.0709 - accuracy: 0.9804 - val_loss: 0.2471 - val_accuracy: 0.9212 Epoch 12/25 26/26 [==============================] - 13s 481ms/step - loss: 0.0582 - accuracy: 0.9779 - val_loss: 0.2217 - val_accuracy: 0.9163 Epoch 13/25 26/26 [==============================] - 13s 486ms/step - loss: 0.0777 - accuracy: 0.9706 - val_loss: 0.2018 - val_accuracy: 0.9163 Epoch 14/25 26/26 [==============================] - 13s 495ms/step - loss: 0.0575 - accuracy: 0.9841 - val_loss: 0.2207 - val_accuracy: 0.9064 Epoch 15/25 26/26 [==============================] - 13s 492ms/step - loss: 0.0552 - accuracy: 0.9804 - val_loss: 0.2480 - val_accuracy: 0.9064 Epoch 16/25 26/26 [==============================] - 13s 486ms/step - loss: 0.0384 - accuracy: 0.9890 - val_loss: 0.2843 - val_accuracy: 0.8916 Epoch 17/25 26/26 [==============================] - 12s 481ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 0.2751 - val_accuracy: 0.8867 Epoch 18/25 26/26 [==============================] - 13s 488ms/step - loss: 0.0201 - accuracy: 0.9963 - val_loss: 0.2511 - val_accuracy: 0.9163 Epoch 19/25 26/26 [==============================] - 13s 493ms/step - loss: 0.0122 - accuracy: 0.9975 - val_loss: 0.3421 - val_accuracy: 0.9212 Epoch 20/25 26/26 [==============================] - 13s 489ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.2460 - val_accuracy: 0.9212 Epoch 21/25 26/26 [==============================] - 13s 485ms/step - loss: 0.0116 - accuracy: 0.9988 - val_loss: 0.3082 - val_accuracy: 0.9113 Epoch 22/25 26/26 [==============================] - 13s 485ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.3040 - val_accuracy: 0.9212 Epoch 23/25 26/26 [==============================] - 13s 488ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.2572 - val_accuracy: 0.9310 Epoch 24/25 26/26 [==============================] - 13s 486ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3655 - val_accuracy: 0.9064 Epoch 25/25 26/26 [==============================] - 13s 484ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.3503 - val_accuracy: 0.9015 . Visualize the Fit History . We can then look at how the loss and accuracy of training and validation sets change while the model is iteratively running. . model_metrics = pd.DataFrame(history.history) model_metrics.plot(figsize = (8,8)) L = plt.legend(loc = 3) L.get_texts()[0].set_text(&#39;Train Loss&#39;) L.get_texts()[1].set_text(&#39;Train Accuracy&#39;) L.get_texts()[2].set_text(&#39;Validation Loss&#39;) L.get_texts()[3].set_text(&#39;Validation Accuracy&#39;) plt.title(&#39;Loss and Accuracy of Trainning Set vs Validation Set&#39;) pass . Evaluate Model with Testing Set . Next, we will test the performance of this neural network on our testing set. . It turns out that the accuracy is 97.79% while the loss is 0.0880. . test_loss, test_acc = model.evaluate(test_imgs) . 6/6 [==============================] - 1s 102ms/step - loss: 0.0880 - accuracy: 0.9778 . Make Prediction . Now we can use our model to make predictions. . First, we will be using a dragonfly image from different data source to predict the label and also the probability of that image belonging to the predicted category. . Then, we will use a dragonfly image from the same data source to predict the its category and also the probability. . import requests from io import BytesIO . def make_img_array(url, img_size): &quot;&quot;&quot;Take in an image from url and return as array for prediction.&quot;&quot;&quot; img = requests.get(url) img = BytesIO(img.content) img = Image.open(img).resize(img_size) img_array = tf.keras.utils.img_to_array(img) img_array = tf.expand_dims(img_array, 0) return img_array . #Dragonfly image from another database predict_img_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/dragonflies_bcc.jpeg&#39; predict_img = make_img_array(predict_img_url, img_size) predict_img /= 255.0 prediction = model.predict(predict_img) score = tf.nn.softmax(prediction[0]) categories = [&#39;beetles&#39;, &#39;cockroach&#39;, &#39;dragonflies&#39;] predict_class = categories[np.argmax(score)] prob_class = np.round(100 * np.max(score), 2) print(f&#39;Based on the model, this image belongs to the class {predict_class} with probability of {prob_class}.&#39;) . Based on the model, this image belongs to the class dragonflies with probability of 41.11. . predict_img_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/dragonflies_insecstimages.jpeg&#39; predict_img = make_img_array(predict_img_url, img_size) predict_img /= 255.0 prediction = model.predict(predict_img) score = tf.nn.softmax(prediction[0]) categories = [&#39;beetles&#39;, &#39;cockroach&#39;, &#39;dragonflies&#39;] predict_class = categories[np.argmax(score)] prob_class = np.round(100 * np.max(score), 2) print(f&#39;Based on the model, this image belongs to the class {predict_class} with probability of {prob_class}.&#39;) . Based on the model, this image belongs to the class dragonflies with probability of 41.1. . From the above results, we can see that our model can successfully predict the category of those two images that we obtained from either the same data source or different data source. . Use SHAP to Explain the Relative Importance of those Features in Image Classification . Since the deep learning method acts like a black box, we don&#39;t have an interpretation of feature importance from our neural network. However, by using SHAP, we can know the impact of every feature to the target variable and the impact is calculated as the shapley values. . In here, we will be using GradientExplainer to look at the impact of those pixels in classifying the category associated with that image. If those pixels have relative great impact, they will be shown in pink while those are making less impact will be shown in blue. . X_train = np.concatenate([x for x, y in train_imgs], axis = 0) Y_train = np.concatenate([y for x, y in train_imgs], axis = 0) . X_test = np.concatenate([x for x, y in test_imgs], axis = 0) Y_test = np.concatenate([y for x, y in test_imgs], axis = 0) . explainer = shap.GradientExplainer(model, X_train) . We randomly sample 15 images in our testing set to comput the shapley values. . np.random.seed(0) sample = np.random.choice(len(X_test), 15, replace = False) #The index will be used later to put labels in the shap image shap_values_test, index = explainer.shap_values(X_test[sample], ranked_outputs = 3) . `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model. . class_names = [&#39;beetles&#39;, &#39;cockroach&#39;, &#39;dragonflies&#39;] # get the names for the classes index_names = np.vectorize(lambda x: class_names[x])(index) shap.image_plot(shap_values_test, X_test[sample], labels = index_names, show = False) . How to interpret the above plot? . Under that label, the pink region represents those pixels are responsible for classifying that image as that label while those blue region shows those pixels have less importance in classifying as that label. . For example, in the second row, we saw there were more pink region under the label of cockroach than the label of beetles and dragonflies. Thus, we can say that image has more features that it will be classified as cockroach. .",
            "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/2021/11/15/Assignment5-Pu-Zeng.html",
            "relUrl": "/2021/11/15/Assignment5-Pu-Zeng.html",
            "date": " • Nov 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A Dashboard Visualization",
            "content": "Data Preparation . Combine Selected Datasets regarding Race Variable . This report is tring to illustrate these following study aims: . How does the number of Doctor degree receivers in each types of study change when stratified by gender or race? | How does the number of Dctor degree receivers in each types of employment change when stratified by race? | How does the number of Dctor degree receivers in each types of debt levels change when stratified by race? | For that, a few data tables were selected to discuss the distribution of race and gender in the variables of: . Major field of study | Employement types | Debt levels | Prior to create the dashboard visualization, there are some data processing need to be done. . First, we combined a few datatables regarding the variable, race. . Race vs Major Field of Study vs Employment Types vs Debt Level . import pandas as pd import numpy as np import warnings warnings.filterwarnings(&#39;ignore&#39;) . debt_race_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/data/raw/Debt_race_2017.csv&#39; . debt_race_df = pd.read_csv(debt_race_url) debt_race_df.head() . Debt level Hispanic or Latino American Indian or Alaska Native Asian Black or African American White More than one race Other race or race not reported Ethnicity not reported . 0 No debt | 833 | 36 | 2,062 | 404 | 11,091 | 445 | 141 | 56 | . 1 $10,000 or less | 249 | 7 | 287 | 156 | 2,004 | 95 | 26 | 9 | . 2 $10,001–$20,000 | 209 | 5 | 212 | 118 | 1,915 | 78 | 22 | 11 | . 3 $20,001–$30,000 | 174 | 6 | 160 | 131 | 1,644 | 65 | 22 | 6 | . 4 $30,001–$40,000 | 104 | 6 | 74 | 112 | 1,141 | 44 | 10 | 5 | . debt_race_df = debt_race_df.melt(id_vars = &#39;Debt level&#39;, var_name = &#39;Ethnicity status&#39;, value_name = &#39;Count&#39;) debt_race_df.head() . Debt level Ethnicity status Count . 0 No debt | Hispanic or Latino | 833 | . 1 $10,000 or less | Hispanic or Latino | 249 | . 2 $10,001–$20,000 | Hispanic or Latino | 209 | . 3 $20,001–$30,000 | Hispanic or Latino | 174 | . 4 $30,001–$40,000 | Hispanic or Latino | 104 | . employment_race_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/data/raw/Employment_race_2017.csv&#39; employment_race_df = pd.read_csv(employment_race_url) employment_race_df.head() . Employment Types Hispanic or Latino American Indian or Alaska Native Asian Black or African American White More than one race Other race or race not reported Ethnicity not reported . 0 All U.S. employment commitments (number) | 898.0 | 41 | 1,121 | 944.0 | 9,969 | 365 | 107.0 | 46.0 | . 1 Academe (%) | 55.3 | 51.2 | 39 | 51.9 | 51.7 | 48.8 | 48.6 | 54.3 | . 2 Government (%) | 10.5 | D | 7.1 | 13.0 | 8.9 | D | 13.1 | 13.0 | . 3 Industry or business (%) | 22.6 | 29.3 | 44.7 | 14.1 | 25.5 | 29 | 26.2 | 23.9 | . 4 Nonprofit organization (%) | 5.1 | D | 6.6 | 8.7 | 7.3 | D | 5.6 | 4.3 | . employment_race_df = employment_race_df.melt(id_vars = &#39;Employment Types&#39;, var_name = &#39;Ethnicity status&#39;) employment_race_df.head() . Employment Types Ethnicity status value . 0 All U.S. employment commitments (number) | Hispanic or Latino | 898 | . 1 Academe (%) | Hispanic or Latino | 55.3 | . 2 Government (%) | Hispanic or Latino | 10.5 | . 3 Industry or business (%) | Hispanic or Latino | 22.6 | . 4 Nonprofit organization (%) | Hispanic or Latino | 5.1 | . race_study_field_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/data/raw/Race_Fine_Field_Study.csv&#39; race_study_field_df = pd.read_csv(race_study_field_url) race_study_field_df.head() . Field of study Hispanic or Latino American Indian or Alaska Native Asian Black or African American White More than one race Other race or race not reported Ethnicity not reported . 0 All fields | 2,540 | 109 | 3,502 | 2,409 | 24,880 | 1,016 | 471 | 864 | . 1 Life sciences | 606 | 25 | 1,014 | 577 | 6,059 | 286 | 107 | 183 | . 2 Agricultural sciences and natural resources | 49 | 2 | 62 | 41 | 674 | 19 | 13 | 42 | . 3 Agricultural economics | 2 | 0 | 4 | 3 | 38 | 0 | 0 | 3 | . 4 Agronomy, horticulture science, plant breeding... | 11 | 1 | 17 | 5 | 150 | 5 | 0 | 13 | . race_study_field_df = race_study_field_df.melt(id_vars = &#39;Field of study&#39;, var_name = &#39;Ethnicity status&#39;, value_name = &#39;Number of count&#39;) race_study_field_df.head() . Field of study Ethnicity status Number of count . 0 All fields | Hispanic or Latino | 2,540 | . 1 Life sciences | Hispanic or Latino | 606 | . 2 Agricultural sciences and natural resources | Hispanic or Latino | 49 | . 3 Agricultural economics | Hispanic or Latino | 2 | . 4 Agronomy, horticulture science, plant breeding... | Hispanic or Latino | 11 | . race_phd_info = ( race_study_field_df. merge(employment_race_df, on = &#39;Ethnicity status&#39;, how = &#39;left&#39; ). merge(debt_race_df, on = &#39;Ethnicity status&#39;, how = &#39;left&#39; ) ) race_phd_info.head() . Field of study Ethnicity status Number of count Employment Types value Debt level Count . 0 All fields | Hispanic or Latino | 2,540 | All U.S. employment commitments (number) | 898 | No debt | 833 | . 1 All fields | Hispanic or Latino | 2,540 | All U.S. employment commitments (number) | 898 | $10,000 or less | 249 | . 2 All fields | Hispanic or Latino | 2,540 | All U.S. employment commitments (number) | 898 | $10,001–$20,000 | 209 | . 3 All fields | Hispanic or Latino | 2,540 | All U.S. employment commitments (number) | 898 | $20,001–$30,000 | 174 | . 4 All fields | Hispanic or Latino | 2,540 | All U.S. employment commitments (number) | 898 | $30,001–$40,000 | 104 | . . Sex vs Major Field of Study . We then will add the sex column to the below data table. . sex_field_study_url = &#39;https://raw.githubusercontent.com/puzeng/BIOSTAT823_FALL21_Pu/main/data/raw/Sex_Fine_Filed_Study.csv&#39; sex_field_study_df = pd.read_csv(sex_field_study_url) sex_field_study_df.head() . Sex and major field of study 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 % change 2008–17 . 0 All doctorate recipientsa | 48,777 | 49,552 | 48,029 | 48,911 | 50,944 | 52,704 | 53,992 | 54,901 | 54,862 | 54,664 | 12.1 | . 1 Life sciences | 11,086 | 11,403 | 11,319 | 11,535 | 11,964 | 12,208 | 12,484 | 12,499 | 12,554 | 12,592 | 13.6 | . 2 Agricultural sciences and natural resources | 1,198 | 1,283 | 1,100 | 1,206 | 1,255 | 1,324 | 1,338 | 1,434 | 1,381 | 1,606 | 34.1 | . 3 Biological and biomedical sciences | 7,797 | 8,025 | 8,046 | 8,152 | 8,322 | 8,355 | 8,868 | 8,788 | 8,873 | 8,477 | 8.7 | . 4 Health sciences | 2,091 | 2,095 | 2,173 | 2,177 | 2,387 | 2,529 | 2,278 | 2,277 | 2,300 | 2,509 | 20.0 | . male_index = sex_field_study_df.iloc[:, 0][sex_field_study_df.iloc[:, 0] == &#39;Male&#39;].index.tolist() female_index = sex_field_study_df.iloc[:, 0][sex_field_study_df.iloc[:, 0] == &#39;Female&#39;].index.tolist() . ALL = sex_field_study_df.iloc[:male_index[0], :] MALE = sex_field_study_df.iloc[male_index[0]:female_index[0], :].reset_index(drop = True) FEMALE = sex_field_study_df.iloc[female_index[0]:, :].reset_index(drop = True) . ALL = ALL.assign(Sex = &#39;All&#39;) ALL = ALL.reindex(ALL.index.drop(0)).reset_index(drop = True) ALL.head() . Sex and major field of study 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 % change 2008–17 Sex . 0 Life sciences | 11,086 | 11,403 | 11,319 | 11,535 | 11,964 | 12,208 | 12,484 | 12,499 | 12,554 | 12,592 | 13.6 | All | . 1 Agricultural sciences and natural resources | 1,198 | 1,283 | 1,100 | 1,206 | 1,255 | 1,324 | 1,338 | 1,434 | 1,381 | 1,606 | 34.1 | All | . 2 Biological and biomedical sciences | 7,797 | 8,025 | 8,046 | 8,152 | 8,322 | 8,355 | 8,868 | 8,788 | 8,873 | 8,477 | 8.7 | All | . 3 Health sciences | 2,091 | 2,095 | 2,173 | 2,177 | 2,387 | 2,529 | 2,278 | 2,277 | 2,300 | 2,509 | 20.0 | All | . 4 Physical sciences and earth sciences | 4,946 | 5,160 | 4,995 | 5,271 | 5,419 | 5,584 | 5,911 | 5,918 | 6,252 | 6,081 | 22.9 | All | . MALE = MALE.assign(Sex = &#39;Male&#39;) MALE = MALE.reindex(MALE.index.drop(0)).reset_index(drop = True) MALE.head() . Sex and major field of study 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 % change 2008–17 Sex . 0 Life sciences | 5,223 | 5,183 | 5,101 | 5,243 | 5,335 | 5,492 | 5,514 | 5,566 | 5,634 | 5,629 | 7.8 | Male | . 1 Agricultural sciences and natural resources | 700 | 722 | 609 | 652 | 698 | 702 | 691 | 746 | 756 | 809 | 15.6 | Male | . 2 Biological and biomedical sciences | 3,861 | 3,834 | 3,823 | 3,878 | 3,891 | 3,941 | 4,088 | 4,103 | 4,159 | 4,014 | 4.0 | Male | . 3 Health sciences | 662 | 627 | 669 | 713 | 746 | 849 | 735 | 717 | 719 | 806 | 21.8 | Male | . 4 Physical sciences and earth sciences | 3,505 | 3,533 | 3,379 | 3,629 | 3,684 | 3,717 | 3,969 | 3,930 | 4,286 | 4,068 | 16.1 | Male | . FEMALE = FEMALE.assign(Sex = &#39;Female&#39;) FEMALE = FEMALE.reindex(FEMALE.index.drop(0)).reset_index(drop = True) FEMALE.head() . Sex and major field of study 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 % change 2008–17 Sex . 0 Life sciences | 5,859 | 6,210 | 6,213 | 6,289 | 6,614 | 6,713 | 6,930 | 6,932 | 6,918 | 6,958 | 18.8 | Female | . 1 Agricultural sciences and natural resources | 498 | 559 | 490 | 554 | 554 | 622 | 646 | 688 | 624 | 795 | 59.6 | Female | . 2 Biological and biomedical sciences | 3,934 | 4,185 | 4,219 | 4,272 | 4,422 | 4,411 | 4,747 | 4,684 | 4,713 | 4,461 | 13.4 | Female | . 3 Health sciences | 1,427 | 1,466 | 1,504 | 1,463 | 1,638 | 1,680 | 1,537 | 1,560 | 1,581 | 1,702 | 19.3 | Female | . 4 Physical sciences and earth sciences | 1,440 | 1,618 | 1,615 | 1,640 | 1,730 | 1,864 | 1,924 | 1,987 | 1,963 | 2,011 | 39.7 | Female | . sex_field_study_df = pd.concat([ALL, MALE, FEMALE],ignore_index=True) sex_field_study_df.head() . Sex and major field of study 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 % change 2008–17 Sex . 0 Life sciences | 11,086 | 11,403 | 11,319 | 11,535 | 11,964 | 12,208 | 12,484 | 12,499 | 12,554 | 12,592 | 13.6 | All | . 1 Agricultural sciences and natural resources | 1,198 | 1,283 | 1,100 | 1,206 | 1,255 | 1,324 | 1,338 | 1,434 | 1,381 | 1,606 | 34.1 | All | . 2 Biological and biomedical sciences | 7,797 | 8,025 | 8,046 | 8,152 | 8,322 | 8,355 | 8,868 | 8,788 | 8,873 | 8,477 | 8.7 | All | . 3 Health sciences | 2,091 | 2,095 | 2,173 | 2,177 | 2,387 | 2,529 | 2,278 | 2,277 | 2,300 | 2,509 | 20.0 | All | . 4 Physical sciences and earth sciences | 4,946 | 5,160 | 4,995 | 5,271 | 5,419 | 5,584 | 5,911 | 5,918 | 6,252 | 6,081 | 22.9 | All | . . Dashboard Visualization . The dashboard is built on Streamlit which is an open-source python framework to build web apps, where you can retrieve from here Pu&#39;s Dashboard Visualization. . In this dashboard, you can explore the relationship between gender variable with major field of study for the PhD awards from year 2008 to year 2017. However, if you want to compare the number of PhD awards by gender, the dashboard only focus on data in year 2017. . You can also look at the race distribution against the variable, major field of study, employment types, and debt levels. . How to interact with the dashboard is detailedly described in the web app. The script, Streamlit_App for building the web app can be retrieved from this GitHub repo Streamlit Web App. The datasets used in the web app are also included in the repo. . Reference . National Science Foundation, National Center for Science and Engineering Statistics. 2018. Doctorate Recipients from U.S. Universities: 2017. Special Report NSF 19-301. Alexandria, VA. Available at https://ncses.nsf.gov/pubs/nsf19301/. .",
            "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/2021/10/25/Assignment4-Pu-Zeng.html",
            "relUrl": "/2021/10/25/Assignment4-Pu-Zeng.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Informative Visualization about Malaria",
            "content": "In this blog, we are going to present information regarding malaria through interactive visualizations, where the datasets about malaria were retrieved from this GitHub repo rfordatascience. . In this blog, we will be using plotly to plot three interactive figures. . The plotly library through Python is a plotting libraray that can produce beautiful interactive visualizations. The reason why to choose this library is because of the following: . Works fine with my blog built on fastpages. | Especially, the Plotly express makes it easy to graph highly interactive figures that have various types. | plotly has clear syntax to work with. | Prepare Dataset for Plotting . import pandas as pd def combine_malaria_dataset(url_deaths, url_deaths_age, url_inc): &quot;&quot;&quot;Combine the three csv files from the github repo for plotting.&quot;&quot;&quot; malaria_deaths = pd.read_csv(url_deaths, sep=&#39;,&#39;) malaria_inc = pd.read_csv(url_inc, sep = &#39;,&#39;) malaria_deaths_age = pd.read_csv(url_deaths_age, sep = &#39;,&#39;, index_col = 0) #To capitalize the first letter in the column names for better join malaria_deaths_age.columns = malaria_deaths_age.columns.str.title() common_cols = list(set(malaria_deaths.columns) &amp; set(malaria_deaths_age.columns) &amp; set(malaria_inc.columns)) malaria_df = ( pd. merge(malaria_deaths, malaria_inc, how = &#39;left&#39;, on = common_cols). merge(malaria_deaths_age, how = &#39;left&#39;, on = common_cols) ) malaria_df.columns = [&quot;Entity&quot;, &quot;Code&quot;, &quot;Year&quot;, &quot;Total_death_per_million&quot;, &quot;Incidence_per_thousand&quot;, &quot;Age_group&quot;, &quot;Deaths_per_age_group&quot;] malaria_df = malaria_df.reset_index(drop = True) return malaria_df . url_deaths = &quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths.csv&quot; url_inc = &#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_inc.csv&#39; url_deaths_age = &#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths_age.csv&#39; malaria_df = combine_malaria_dataset(url_deaths, url_deaths_age, url_inc) malaria_df.head() . Entity Code Year Total_death_per_million Incidence_per_thousand Age_group Deaths_per_age_group . 0 Afghanistan | AFG | 1990 | 6.80293 | NaN | Under 5 | 184.606435 | . 1 Afghanistan | AFG | 1990 | 6.80293 | NaN | 70 or older | 10.728850 | . 2 Afghanistan | AFG | 1990 | 6.80293 | NaN | 5-14 | 53.352844 | . 3 Afghanistan | AFG | 1990 | 6.80293 | NaN | 15-49 | 414.709676 | . 4 Afghanistan | AFG | 1990 | 6.80293 | NaN | 50-69 | 60.541746 | . Interactive Visualization for Malaria Data . Malaria Total Death Cases in Country Levels between 1990 to 2016 . One interesting aspect to look at this malaria dataset is to monitor the change of malaria total death cases across a time series at different country levels. Thus, we generated a time series plot of total deaths between year 1990 to 2016. This visual is interactive in a way that we can self-define a couple of countries as the observation. . To interact with the plot, user could perform the following actions: . Double click at the legend to select the countries as many as they want to look at. | Double click at the legend to exist the selection mode. | Scroll up and down the legend to include countries into the selection. | Move the cursor on the line to know the value corresponding to the cursor location. | Crop an area on the figure to zoom in. | import dash_core_components as dcc import dash_html_components as html from dash.dependencies import Input, Output import plotly.express as px from plotly import offline from plotly.offline import iplot from IPython.display import HTML fig = px.line(malaria_df, x = &#39;Year&#39;, y = &#39;Total_death_per_million&#39;, color = &#39;Entity&#39;, title = &#39;Total death cases between year 1990 to 2016 at different country levels&#39;, labels={&#39;Entity&#39;: &#39;Country&#39;, &#39;Year&#39;: &#39;Year&#39;, &#39;Total_death_per_million&#39; : &#39;Total Number of Deaths per Million&#39;}) offline.iplot(fig) . Malaria Incidence Rate between Year 2000 to 2015 . The second visualization is the change of incidence rate between year 2000 to year 2015 at different country levels. The reason why the time period is inconsistent with the total dealth figure is because the dataset only contains information about incidence rate between year 2000 to 2015. . This time series plot for malaria incidence rate can help understand and compapre the rate of new cases for different countries. The actions to interactive with the plot are the same as above which they are: . Double click at the legend to select the countries as many as they want to look at. | Double click at the legend to exist the selection mode. | Scroll up and down the legend to include countries into the selection. | Move the cursor on the line to know the value corresponding to the cursor location. | Crop an area on the figure to zoom in. | fig = px.line(malaria_df.dropna(), x = &#39;Year&#39;, y = &#39;Incidence_per_thousand&#39;, color = &#39;Entity&#39;, title = &quot;Incidence rate between year 2000 to 2015 at different country levels&quot;, labels={&#39;Entity&#39;: &#39;Country&#39;, &#39;Year&#39;: &#39;Year&#39;, &#39;Incidence_per_thousand&#39; : &#39;Malaria Incidence per Thousand&#39;}) offline.iplot(fig) . Death Cases between Age Groups . Another way to look at the death cases is to look at the death cases between different age groups regardless of the country information. . For that, we generated a box plot that is interactive. The reason why we choose to use box plot is that the deaths in different year can be plotted in one age group so that the change within that age group between different years can also be visualized. . The actions to interactive with the plot are the same as above which they are: . Double click at the legend to select the countries as many as they want to look at. | Double click at the legend to exist the selection mode. | Scroll up and down the legend to include countries into the selection. | Move the cursor on the line to know the value corresponding to the cursor location. | Crop an area on the figure to zoom in. | fig = px.box(malaria_df, x = &#39;Age_group&#39;, y = &#39;Deaths_per_age_group&#39;, color = &#39;Year&#39;, labels = {&#39;Age_group&#39;: &#39;Age group&#39;, &#39;Deaths_per_age_group&#39;: &#39;Deaths per age group&#39;}, category_orders = {&#39;Age_group&#39;: [&#39;Under 5&#39;, &#39;5-14&#39;, &#39;15-49&#39;, &#39;50-69&#39;, &#39;70 or older&#39;]}, title = &quot;Death cases at different age groups between year 1990 to 2016&quot;) #To format the y axis expression fig.update_layout(yaxis=dict(tickformat=&quot;3,.0f&quot;)) offline.iplot(fig) .",
            "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/2021/10/01/Assignmeng3-PuZeng.html",
            "relUrl": "/2021/10/01/Assignmeng3-PuZeng.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Number Theory and a Google Recruitment Puzzle",
            "content": "The main goal of this project is to find the first 10-digit prime in the decimal expansion of 17$ pi$. . This blog post is going to solve this question by using three helper functions. . This blog can be retrieved in here: Pu&#39;s Blog. . In addition, this notebook is kept in here: Pu&#39;s Repo under the folder _notebooks. . Let&#39;s load all the required packages first. . import math import numpy as np import re import sympy . Expand the mathematical expreesion . First, we are going to generate an arbitrary large expression of a mathematical expression like $ pi$. For that, a helper function called expansion_expression will help to do that. . As the input will be a string that contains the mathematical expression and its coefficient if has, we are going to use regular expression to extract the coefficient of the mathematical expression if there is one. . Next, we will expand the mathematical expression first to have a number of self-defined digits before generate the multiple of the mathematical expression. . Then, the multiple of the mathematical expression is generated by multiplying the coefficient with the mathematical expression. . The following is how the helper function is programmed. . def expansion_mathematical_expression(math_expression, num_digits): &quot;&quot;&quot;Genearate an arbitrary expression of mathematical expression.&quot;&quot;&quot; #Coefficient are the any digits in numeric before the mathematical expression. pattern = re.compile(r&#39;[0-9]+&#39;) if &quot;pi&quot; in math_expression: #Generate the mathematical expression in self-defined digits expanded_expression = sympy.N(sympy.pi, num_digits) if len(pattern.findall(math_expression)) == 1: times = float(pattern.match(math_expression)[0]) expanded_expression = str(expanded_expression * times) else: expanded_expression = str(expanded_expression) if &quot;e&quot; in math_expression: #Generate the mathematical expression in self-defined digits expanded_expression = sympy.N(sympy.E, num_digits) if len(pattern.findall(math_expression)) == 1: times = float(pattern.match(math_expression)[0]) expanded_expression = str(expanded_expression * times) else: expanded_expression = str(expanded_expression) #Ignore the period expanded_expression = re.sub(r&#39; .&#39; , &quot;&quot; ,expanded_expression) return expanded_expression . Check prime number . The second helper function that we are going to implement is the function to check whether the number is a prime or not, called is_prime. . We are not going to solve by brute force method which is to use all numbers from 2 to itself to divide that number. However, we can analyze this problem first and solve with the analytical method. . The product combinations of a number can be divided into two halves where they are mirroring to each other. For example, number 36 has the following combinations: . 1 * 36 | 2 * 18 | 3 * 12 | 4 * 9 | 6 * 6 | 9 * 4 | 12 * 3 | 18 * 2 | 36 * 1 | We can notice that the first 4 pairs are mirroring with the last 4 pairs while the fifth pair is the mirroring line. Thus, we can generalize this question by checking whether the number can be divided by the factors up to its square root so that we don&#39;t need to check all the factors. . The codes below show how the analytical method is implemented: . def is_prime(check_window): &quot;&quot;&quot;Check whether the number is a prime or not.&quot;&quot;&quot; check_window = int(check_window) # Check whether this number can be divided by any factors up to its sqrt(check_window) for i in range(2, int(math.sqrt(check_window))+1): if check_window % i == 0: return False return True . Slide window . The third helper function is to generate all the sliding windows on the mathematical expression through looping through the expression. The length of the window is depending on the number of digits that the user is trying to check for the prime. . This is accomplished by appending all the sliding windows to the other list while sliding down the string. . Below shows how the above arguments are implemented: . def slide_window(width, input_text): &quot;&quot;&quot;Generate all sliding windows for a mathematical expression based on the customized length.&quot;&quot;&quot; slide_windows = list() while len(input_text) &gt;= width: slide_windows.append(input_text[:width]) input_text = input_text[1:] return slide_windows . Assemble to answer the question . As we go through all the helper functions that are used in answering the question, we now can assemble them as in one function to help solve the puzzle. . The main function will take two arguments: . num_digits: an integer for defining the number of digits to expand on the mathematical expression. | math_expression: a string that are going to be expanded based on which mathematical expression is input. | width_side_window: an integer for indicating how many digits that the prime is going to be. | Within the big function called ten_digits_primes_in_expression, we will first generate an arbitrary large expression of the corresponding mathematical expression by self definition through the function, expansion_mathematical_expression. . Then, we are generating all the sliding windows based on a self-defined length which is also equivalent to the number of digits in the prime, by calling the function slide_window. . To tell whether the slide window is a prime a not, we need to call function is_prime. If it is a prime, this function will return True and otherwise. . Once, we detect that if the slide window is the prime, we will immediatelly return as the digits in the slide window. If not, we will go to the next slide window. . Below shows how the above arguments are implemented and how to use the function: . def ten_digits_primes_in_expression(num_digits, math_expression, width_slide_window): &quot;&quot;&quot;Extrat first 10 prime digits in a mathematical expression.&quot;&quot;&quot; #Expand the expression in self-defined digits expanded_expression = expansion_mathematical_expression(math_expression, num_digits) #Generate all the slide windows by self-defined width all_slide_windows = slide_window(10, expanded_expression) #Check whether the current window is prime or not for ele in all_slide_windows: if is_prime(ele) == True: return ele . first_prime = ten_digits_primes_in_expression(300, &quot;17pi&quot;, 10) print(f&quot;The first 10-digit prime in the decimal expansion of 17 u03C0 is {first_prime}.&quot;) . The first 10-digit prime in the decimal expansion of 17π is 8649375157. . For contributors: Testing . import unittest class TestNotebook(unittest.TestCase): def test_expansion_mathematical_expression(self): self.assertEqual(expansion_mathematical_expression(&quot;pi&quot;, 10), &#39;3141592654&#39;) self.assertEqual(expansion_mathematical_expression(&quot;e&quot;, 10), &#39;2718281828&#39;) def test_is_prime(self): self.assertEqual(is_prime(2), True) self.assertEqual(is_prime(111), False) self.assertEqual(is_prime(5), True) self.assertEqual(is_prime(596), False) def test_slide_window(self): self.assertEqual(slide_window(5, &quot;12345&quot;), [&quot;12345&quot;]) self.assertEqual(slide_window(1, &quot;12345&quot;), [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]) def test_ten_digits_primes_in_expression(self): self.assertEqual(ten_digits_primes_in_expression(300, &quot;e&quot;, 10), &#39;7427466391&#39;) unittest.main(argv = [&#39;&#39;], verbosity = 2, exit = False) . test_expansion_mathematical_expression (__main__.TestNotebook) ... ok test_is_prime (__main__.TestNotebook) ... ok test_slide_window (__main__.TestNotebook) ... ok test_ten_digits_primes_in_expression (__main__.TestNotebook) ... ok - Ran 4 tests in 0.015s OK . &lt;unittest.main.TestProgram at 0x7fd1e715e850&gt; . Supplementary materials . Besides to check whether a number is a prime or not by the above analytical method, we can also solve this question by another analytical approach, Sieve of Eratosthenes. . In general, the mechanism follows that within a range of number, the multiples of 2, 3, 4, and all the way up to the square root of the number will be crossed out. . The implementation then will follow: . Initially, generate a boolean list to cast those primes by the length of that number. | We will then cross out the multiples of 2 at the beginning and stop untill the multiples of 2 is greater that number. | Then, cross out the multiples of 3. | Cross out the multiples of factors that are all the way up till the square root of that number. | Generate an array contains a range of that number. | Map the boolean list to the array to get all the primes of the range. | If that number itself is within the list, then return True or otherwise. | This method based on Siev of Eratosthene is much faster than the method used above since the complexity is O(N* log(log(N))) compared to O(sqrt(N)). . def is_prime_sieve_eratosthenes(check_window): &quot;&quot;&quot;Check whether the number is a prime or not based on Sieve Of Eratosthenes.&quot;&quot;&quot; #Create a list of True initially #Once find the multiples, update True as False check_window = int(check_window) prime_list = [True for num in range(check_window+1)] iterate_num = 2 while iterate_num * iterate_num &lt;= check_window: if prime_list[iterate_num] == True: for i in range(iterate_num * iterate_num, check_window+1, 2): prime_list[i] = False iterate_num += 1 target_list = np.array(range(0, check_window+1)) target_list = target_list[prime_list] return check_window in target_list print(f&quot;5 is a prime? {is_prime_sieve_eratosthenes(5)}&quot;) . 5 is a prime? True .",
            "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/2021/09/17/Assignment2_PuZeng.html",
            "relUrl": "/2021/09/17/Assignment2_PuZeng.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Math is Fun - BIOSTAT 823 Assignment",
            "content": "This post will consist my solutions for 3 questions taken from Euler Project which they are: . 1. How many reversible numbers are there below one-billon? (ID: 145, solved by 16438 people) 2. Permuted multiples. (ID: 52, solved by 65547 people) 3. Summation of primes. (ID: 10, solved by 330347 people) . This post can be retrieved from Pu&#39;s Blog for Biostat823. And the post is auto-converted by Fastpages based on a jupyter nootebook where is kept in the Pu&#39;s BIOSTAT823 repo under the folder of _notebooks. . Repository can be accessd through this website: https://github.com/puzeng/BIOSTAT823_Blog_PuZeng. . The post can be accessed from here: https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/. . 1. How many reversible numbers are there below one-billion? . &quot;Some positive integers n have the property that the sum [ n + reverse(n) ] consists entirely of odd (decimal) digits. For instance, 36 + 63 = 99 and 409 + 904 = 1313. We will call such numbers reversible; so 36, 63, 409, and 904 are reversible. Leading zeroes are not allowed in either n or reverse(n). . There are 120 reversible numbers below one-thousand. . How many reversible numbers are there below one-billion ($10^9$)?&quot; . This is a question posted on the Euler Project which was solved by 16435 people so far. I&#39;m going to walk you through the analytical process that I took for solving the problem. . This a question that can be solved by a brute force method which means that it can be solved by checking whether the number meet the property that the sum of itself and revsersed number will only have odd digits. Below shows the brute force approach for this question. . Brute Force Approach . First, we need a helper function (is_reversible(input_number)) to identify whether the number is meeting the property or not which is the sum of itself and the reversed number only consists of odd digits. Within the helper function, if the number can be divided by 10, then the number immediately disqualified for be a reversible number. Then, we need to generate the sum of itself and the reversed number. To reverse the number, we can convert the number into a string to reverse it by using [::-1], lastly converting to integer again. . Once we generate the sum, we can check the digits of the sum by extracting each digit of the number. If the digit is an even digit, then the number is disqualified. But if the digit is odd, we keep checking on the next digit. For a number is reversible, the number can successfully go through the while loop and return True at the end line of the function. . Then, we use this helper function inside the function for counting the number of reversible numbers within a range that user defined. The count function starts with generating a list of number that is within the user defined range. Next, we are going to use a for loop to loop through the list to check each number of the list by using the helper function. If it is a reversible number, we increase the count by 1. The count function lastly will return the count which tells us how many reversible numbers within the defined range. . def is_reversible(input_number): &quot;&quot;&quot;Helper function for checking whether the numeber is reversible or not.&quot;&quot;&quot; if input_number % 10 == 0: return False reversed_number = int(str(input_number)[::-1]) sum_n_reverse = reversed_number + input_number while sum_n_reverse &gt; 0: if (sum_n_reverse % 10) % 2 == 0: return False sum_n_reverse //= 10 return True def count_reversible_numbers(input_numbers): &quot;&quot;&quot;Count the number of reversible numbers below one billion.&quot;&quot;&quot; input_list = range(input_numbers) #target_list = list() count = 0 for number in input_list: if is_reversible(number): #target_list.append(number) count += 1 #return(target_list) return count count_reversible_numbers(1000) . 120 . But since we are dealing with a range like one billion numbers, the brute force method is very insufficient in computation. Therefore, we can analyze the question by finding a pattern in the base cases. . We can approach this question by analyzing the different scenarios causing by the addition between digits. I will give detailed explanation about how this approach works in the following. . Analytical Approach . range(10^1) . In the range of numbers are in 1 digits, those number are all disqualified to be reversible since the addition of the numbers between 1-9 to itself is an even number. Therefore, we can&#39;t find any reversible number when the numbers are in 1 digit. . There is no solution. . range(10^2) . When the numbers are in 2 digits, we can use ab to represent the number in 2 digits. Then, the sum of itself and the reversed number is represented by: a+b_b+a. To meet the property, a+b must be an odd number and cannot have a carryover. In the other words, a+b &lt; 10 and a+b is an odd number. . There are 20 pairs of a and b to meet this requirement. . range(10^3) . When the numbers are in 3 digits, we us abc to represent. Then, the sum of itself and the reversed number = a+c_b+b_c+a. We can refer c+a as the outer pair and b+b as the inner pair. . Since the inner pair is the addition to itself. Like we analyzed in the 1 digit scenario, the addition to itself will always produce an even number. Therefore, the middle pair needs a 1 from the carryover of the pair c+a to become an odd number. In addition, it implies that the middle pair must not have a carryover otherwise the carryover will cause the pair a+c become an even number based on the fact that a+c needs to be an odd number. . Thus, we restrict those solutions to meet the following requirements: . b+b &lt; 10 and b+b is an even number. | 20 &gt; a+c &gt; 10 and a+c is an odd number. | Therefore, there are 5 * 20 = 100 solutions. . range(10^4) . We can use abcd to represent numbers in 4 digits. Then, the sum can be written as: a+d_b+c_c+b_d+a. We are referring a+d as the outer pair and b+c as the inner pair. . This is like the scenario that we analyzed in the 2 digits case that d+a must not have a carryover and has to be an odd number and so does the pair, c+b. However, since the pair c and b is in the middle, c+b can be 0. . Thus, the solutions need to meet those requirements: . a+d &lt; 10 and a+b is an odd number. | c+b &lt; 10 and c+b is an odd number and can be 0. | Therefore, there are 20*30 = 600 solutions. . range(10^5) . Numbers in 5 digits can be represented as abcde. Then, the sum = a+e_b+d_c+c_d+b_e+a. . Since c+c is the addition to itself, it will generate the even number. It also implied that c+c will borrow the 1 from the carryover of d+b. And this also tells us that a+e will take the 1 from the carryover of b+d to become an odd number. . Thus, we have the follow restrictions: . c+c is an even number. | b+d &gt; 10 and is an odd number. | a+e is an odd number. | However, there is no solution since the a+e needs to be an odd number and needs to take 1 from the carryover of b+d. . range(10^6) . Numbers in 6 digits can be represented as abcdef. The sum = a+f_b+e_c+d_d+c_e+b_f+a. . This scenario is also pretty much like the 2 digits case where: . c+d &lt; 10 and c+d can be an odd number including 0. | b+e &lt;10 and has to be an odd number including 0. | a+f &lt;10 and has to be an odd number excluding 0. | Therefore, we have 20*30^2 solutions. . range(10^7) . Numbers in 7 digits can be represented as abcdefg. The sum = a+g_b+f_c+e_d+d_e+c_f+b_g+a. . The solutions must meet the following restrictions: . d+d &lt; 10 and must be an even number. | c+e &gt; 10 and must be an odd number including 0. | f+b &lt; 10 and must be an odd number including 0. | a+g &lt; 10 and must be an odd number excluding 0. | Thus, there are 5 20 20 25 = 100 500 solutions. . range(10^8) . Numbers in 8 digits can be represented as abcdefgh. The sum = a+h_b+g_c+f_d+e_e+d_f+c_g+b_h+a. . The solutions must meet the following restrictions like the case in 2 or 4 or 6 digits: . a+h &lt; 10 and must be an odd number excluding 0. | b+g, c+f, and d+e &lt; 10 and must be an odd number including 0. | Thus, there are 20 * 30^3 solutions. . range(10^9) . Numbers in 9 digits can be represented as abcdefghi. The sum = a+i_b+h_c+g_d+f_e+e_f+d_g+c_h+b_i+a. . This case is pretty much similar like the case for 5 digits that we cannot find solutions. The reason why is that e+e needs the 1 from the carryover of d+f since the addition of e itself will only generate the even number. However, it also means that c+g, h+b are both &gt;10 and they are even numbers. Thus, since a+i has to be an odd number, a+i will be changed to even due to the carryover from b+h. . Thus, there is no solution. . Analytical Approach: Summary . Since we have done with analyzing those base cases from 1 digit to 9 digits, we can generalize the solutions based on the pattern. . For the number of digits in 1, 5, and 9, solutions = 0. For number of digits in 2, 4, 6, and 8, solutions = 20 30^n where n = # of digits / 2 - 1. For number of digits in 3 and 7, solutions = 100 500^n where n = (# of digits -3) / 4. . Based on the pattern, we can transform it into a function. . First, we need to figure out what those cases in that number range by finding the maximum numbers of digits that this numer range can hold. For that, we just need to take the log of the input number of base 10. And then, we evaluate the digit number based on the pattern that we found: . If the digit number can be divided by 2, then the count = 20 * 30^(digits/2-1). | If the digit number can be expressed as 4i+3 for i = 0,1,2,3,..., then the count = 100 500^[(digits-3)/4]. | If the digit number failed to meet the above two requirements, then there is no solution. | Below is the function for counting the number of reversible numbers within a defined number range based on analytical approach: . import math def count_reversible_nums(num_range): &quot;&quot;&quot;Count the number of the reversible numbers within a input-range.&quot;&quot;&quot; pow_number = int(math.log10(num_range)) count = 0 for power in range(2,pow_number + 1): if power % 2 == 0: count += 20 * math.pow(30 ,power / 2 - 1) if (power - 3) % 4 == 0: count += 100 * math.pow(500, (power - 3)/4) return int(count) count_reversible_nums(1000000000) . 608720 . 2. Permuted Multiples . &quot;It can be seen that the number, 125874, and its double, 251748, contain exactly the same digits, but in a different order. . Find the smallest positive integer, x, such that 2x, 3x, 4x, 5x, and 6x, contain the same digits.&quot; . This is a question posted on the Euler Project which was solved by 65547 people so far (ID: 52). I&#39;m going to walk you through the process that I took for solving the problem. . We can start with number 2 and move up by 1 until the smallest positive integer that satisfies the requirement is found. To check the requirement, we need to generate several numbers that are corresponding to 2-6 times of that positive integer. . And then, a helper function will check whether the positive integer and the corresponding multiple of that number contain the same digits or not. Within the helper function, the checking process will be accomplished by comparing the two sorted numbers after converting them into strings. . Once we confirm that those multiples of that positive integer have the same digits as the positive integer itself, we will stop the increments and return to that current number. . Below is showing how the solution is coded into a function. . def same_digits(num1, num2): &quot;&quot;&quot;Helper function to check whether two numbers contain same digits.&quot;&quot;&quot; if sorted(str(num1)) == sorted(str(num2)): return True return False def permuted_multiple(): &quot;&quot;&quot;Find the smallest positive integer x, such that 2x, 3x, 4x, 5x, and 6x, contain the same digits.&quot;&quot;&quot; found = False num = 2 while not found: two_times = num * 2 three_times = num * 3 four_times = num * 4 five_times = num * 5 six_times = num * 6 if (same_digits(num, two_times) and same_digits(num, three_times) and same_digits(num, four_times) and same_digits(num, five_times) and same_digits(num, six_times) ): found = True return num num += 1 return num permuted_multiple() . 142857 . 3. Summation of Primes . &quot;The sum of the primes below 10 is 2 + 3 + 5 + 7 = 17. . Find the sum of all the primes below two million.&quot; . This is a question posted on the Euler Project which was solved by 330347 people so far (ID: 10). . To approach this question, we can start with creating a list that contains every positive integers below the number range and then looping through the list to find all the primes. Since we are looping through the number list, we can exclude some integers from the list prior to the looping process to save some energy. . As we know that the prime is a number that can only be divided by 1 or itself, therefore any even number greater than 2 will automatically excluded from being a prime number since even numbers can be divided by 2. Thus, we can remove those even numbers greater than 2 from the number list. . In addition, any number ends in 5 can also be removed from the list since those numbers can be divided by 5. . Then, we will send the cleaned number list into a for loop to loop through the remainning numbers in the list and sum up all the primes in the list. . While we are looping through the number list, we will need a helper function to check whether the current number is a prime or not. If we find the prime, we will add that number to the sum. . Within the helper function, we are trying to check whether the input number is a prime or not. This is accomplished by dividing the factors of the input number into two halves. These two halves of factors are mirroring with each other if the number is not prime. We can use number 64 as the illustration. 64 can be obtained from the multiplication of the following pairs: . 1 * 64 | 2 * 32 | 4 * 16 | 8 * 8 | 16 * 4 | 32 * 2 | 64 * 1 | We can see that the square root of the number is the number can help to divide the factors into two halves. In addition, you may also think what if the square root of number is not an integer. We will then use the nearest integer of the square root as the mirror line. Thus, we can generate a list of factors that are below the mirror number to check whether the input number can be divided by those factors. As long as we find a factor of that number besides 1 and itself, we will return False as the indication of that number is not a prime and the function that generates the sum of the primes will increment by 1 to move to the next number in the list. . Below shows how the solution is coded into functions to solve the question: . import math def is_prime(num): &quot;&quot;&quot;Helper function to check whether the number is a prime or not.&quot;&quot;&quot; for i in range(2, int(math.sqrt(num))+1): if (num % i == 0): return False return True def sum_primes_below_num_range(num_range): &quot;&quot;&quot;Sum all the primes below the input number range.&quot;&quot;&quot; # remove all the even number above 3 but below 2 million list_nums = range(3,num_range,2) #remove all the numbers above 5 but end in 5 from the list removed_nums = range(15,num_range,10) list_nums = list(set(list_nums) - set(removed_nums)) sum = 2 for ele in list_nums: if is_prime(ele) == True: sum += ele return sum sum_primes_below_num_range(2000000) . 142913828922 .",
            "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/2021/09/03/Assignment1_PuZeng.html",
            "relUrl": "/2021/09/03/Assignment1_PuZeng.html",
            "date": " • Sep 3, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://puzeng.github.io/BIOSTAT823_Blog_PuZeng/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}